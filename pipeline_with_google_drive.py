#!/usr/bin/env python3
"""
Production Pipeline —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ Google Drive
–ó–∞–≥—Ä—É–∂–∞–µ—Ç CSV —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä—è–º–æ –≤ –≤–∞—à Google Drive
"""

import pandas as pd
import requests
import easyocr
from PIL import Image
import io
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import argparse
import sys
import json
import time
from datetime import datetime
import os
import re
import openai
import base64
import logging
from pathlib import Path

# Google Drive API
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.oauth2.service_account import Credentials
import tempfile

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class GoogleDriveUploader:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤ –≤ Google Drive"""
    
    def __init__(self, credentials_path: str, folder_name: str = "Kids Supplements Results"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Google Drive API
        
        Args:
            credentials_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É credentials.json
            folder_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –≤ Google Drive –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        self.credentials_path = credentials_path
        self.folder_name = folder_name
        self.service = None
        self.folder_id = None
        
        self._initialize_drive_service()
        self._create_or_get_folder()
        
    def _initialize_drive_service(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Google Drive API —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            # –û–±–ª–∞—Å—Ç–∏ –¥–æ—Å—Ç—É–ø–∞ –¥–ª—è Google Drive
            SCOPES = ['https://www.googleapis.com/auth/drive.file']
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º credentials
            if not os.path.exists(self.credentials_path):
                raise FileNotFoundError(f"–§–∞–π–ª credentials –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.credentials_path}")
            
            credentials = Credentials.from_service_account_file(
                self.credentials_path, 
                scopes=SCOPES
            )
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å
            self.service = build('drive', 'v3', credentials=credentials)
            logger.info("‚úÖ Google Drive API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Google Drive API: {e}")
            raise
    
    def _create_or_get_folder(self):
        """–°–æ–∑–¥–∞–µ—Ç –∏–ª–∏ –Ω–∞—Ö–æ–¥–∏—Ç –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ø–∞–ø–∫—É
            query = f"name='{self.folder_name}' and mimeType='application/vnd.google-apps.folder'"
            results = self.service.files().list(q=query).execute()
            folders = results.get('files', [])
            
            if folders:
                self.folder_id = folders[0]['id']
                logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ø–∞–ø–∫–∞: {self.folder_name}")
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –ø–∞–ø–∫—É
                folder_metadata = {
                    'name': self.folder_name,
                    'mimeType': 'application/vnd.google-apps.folder'
                }
                
                folder = self.service.files().create(body=folder_metadata).execute()
                self.folder_id = folder.get('id')
                logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –ø–∞–ø–∫–∞: {self.folder_name}")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–±–æ—Ç—ã —Å –ø–∞–ø–∫–æ–π: {e}")
            self.folder_id = None
    
    def upload_file(self, file_path: str, drive_filename: str = None) -> Optional[str]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª –≤ Google Drive
        
        Args:
            file_path: –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            drive_filename: –ò–º—è —Ñ–∞–π–ª–∞ –≤ Drive (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫–∞–∫ –ª–æ–∫–∞–ª—å–Ω—ã–π)
            
        Returns:
            URL —Ñ–∞–π–ª–∞ –≤ Google Drive –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            if not os.path.exists(file_path):
                logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                return None
                
            if not drive_filename:
                drive_filename = os.path.basename(file_path)
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
            file_metadata = {
                'name': drive_filename,
                'parents': [self.folder_id] if self.folder_id else []
            }
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
            media = MediaFileUpload(file_path, resumable=True)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª —Å —Ç–∞–∫–∏–º –∏–º–µ–Ω–µ–º
            existing_file = self._find_existing_file(drive_filename)
            
            if existing_file:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª
                file = self.service.files().update(
                    fileId=existing_file['id'],
                    media_body=media
                ).execute()
                logger.info(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω —Ñ–∞–π–ª –≤ Google Drive: {drive_filename}")
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª
                file = self.service.files().create(
                    body=file_metadata,
                    media_body=media,
                    fields='id'
                ).execute()
                logger.info(f"üì§ –ó–∞–≥—Ä—É–∂–µ–Ω –Ω–æ–≤—ã–π —Ñ–∞–π–ª –≤ Google Drive: {drive_filename}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Ñ–∞–π–ª
            file_id = file.get('id')
            
            # –î–µ–ª–∞–µ–º —Ñ–∞–π–ª –¥–æ—Å—Ç—É–ø–Ω—ã–º –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            self.service.permissions().create(
                fileId=file_id,
                body={'role': 'reader', 'type': 'anyone'}
            ).execute()
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Å—ã–ª–∫—É
            drive_url = f"https://drive.google.com/file/d/{file_id}/view"
            logger.info(f"üîó –°—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª: {drive_url}")
            
            return drive_url
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Google Drive: {e}")
            return None
    
    def _find_existing_file(self, filename: str) -> Optional[Dict]:
        """–ò—â–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ"""
        try:
            parent_query = f"'{self.folder_id}' in parents" if self.folder_id else ""
            name_query = f"name='{filename}'"
            
            query = f"{name_query} and {parent_query}" if parent_query else name_query
            
            results = self.service.files().list(q=query).execute()
            files = results.get('files', [])
            
            return files[0] if files else None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–∞: {e}")
            return None
    
    def get_folder_url(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –ø–∞–ø–∫—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
        if self.folder_id:
            return f"https://drive.google.com/drive/folders/{self.folder_id}"
        return "–ü–∞–ø–∫–∞ –Ω–µ —Å–æ–∑–¥–∞–Ω–∞"


class OpenAISupplementFactsAnalyzer:
    """AI –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ OpenAI Vision API (–∫–æ–ø–∏—è –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ pipeline)"""
    
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        self.cache = {}
        self.total_cost = 0.0
        self.request_count = 0
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∫—ç—à–∞
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        logger.info("‚úÖ OpenAI Vision API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _extract_image_id(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ Amazon URL"""
        try:
            match = re.search(r'/([A-Z0-9]{10,11})[._-]', url)
            return match.group(1) if match else ""
        except:
            return ""
    
    def _download_image(self, url: str, max_retries: int = 3) -> bytes:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å retry –ª–æ–≥–∏–∫–æ–π"""
        for attempt in range(max_retries):
            try:
                response = requests.get(url, timeout=30, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                response.raise_for_status()
                
                if len(response.content) < 1000:
                    raise ValueError("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–æ–µ")
                
                return response.content
                
            except Exception as e:
                logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt == max_retries - 1:
                    raise
                time.sleep(2 ** attempt)
    
    def _encode_image_base64(self, image_bytes: bytes) -> str:
        """–ö–æ–¥–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64"""
        return base64.b64encode(image_bytes).decode('utf-8')
    
    def _save_cache(self, image_id: str, data: dict):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à"""
        if image_id:
            cache_file = self.cache_dir / f"{image_id}.json"
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _load_cache(self, image_id: str) -> Optional[dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞"""
        if not image_id:
            return None
        
        cache_file = self.cache_dir / f"{image_id}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return None
    
    def analyze_supplement_facts(self, url: str, product_name: str = "", brand: str = "") -> tuple:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ Supplement Facts —á–µ—Ä–µ–∑ OpenAI Vision
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (ingredients, dosages, age_group, form)
        """
        logger.info(f"ü§ñ –ê–Ω–∞–ª–∏–∑ Supplement Facts: {url[:50]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        image_id = self._extract_image_id(url)
        cached_result = self._load_cache(image_id)
        
        if cached_result:
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –≤ –∫—ç—à–µ: {image_id}")
            return (
                cached_result.get("ingredients", ""),
                cached_result.get("dosages", ""),
                cached_result.get("age_group", ""),
                cached_result.get("form", "")
            )
        
        try:
            # –°–∫–∞—á–∏–≤–∞–µ–º –∏ –∫–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_bytes = self._download_image(url)
            image_base64 = self._encode_image_base64(image_bytes)
            
            # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
            system_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —ç—Ç–∏–∫–µ—Ç–æ–∫ –ø–∏—â–µ–≤—ã—Ö –¥–æ–±–∞–≤–æ–∫ –∏ –≤–∏—Ç–∞–º–∏–Ω–æ–≤. 

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ Supplement Facts –∏ –∏–∑–≤–ª–µ–∫–∏ —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:

1. –ò–ù–ì–†–ï–î–ò–ï–ù–¢–´: –í—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –∏–∑ —Ç–∞–±–ª–∏—Ü—ã Supplement Facts
2. –î–û–ó–ò–†–û–í–ö–ò: –¢–æ—á–Ω—ã–µ –¥–æ–∑–∏—Ä–æ–≤–∫–∏ —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞
3. –í–û–ó–†–ê–°–¢–ù–ê–Ø –ì–†–£–ü–ü–ê: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–æ–∑—Ä–∞—Å—Ç–µ (2+, 4+, 6+ –∏ —Ç.–¥.)
4. –§–û–†–ú–ê –í–´–ü–£–°–ö–ê: –¢–∏–ø –ø—Ä–æ–¥—É–∫—Ç–∞ (Gummies, Chewable, Tablets, Capsules, Liquid, Drops, Powder, Softgels)

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
{
  "ingredients": "—Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é",
  "dosages": "–ø–∞—Ä—ã '–ò–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç: –¥–æ–∑–∏—Ä–æ–≤–∫–∞ –µ–¥–∏–Ω–∏—Ü–∞' —á–µ—Ä–µ–∑ —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π",
  "age_group": "–≤–æ–∑—Ä–∞—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ X+",
  "form": "—Ç–æ—á–Ω–∞—è —Ñ–æ—Ä–º–∞ –≤—ã–ø—É—Å–∫–∞"
}

–í–ê–ñ–ù–û:
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –æ—Å—Ç–∞–≤—å –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É ""
- –í–∫–ª—é—á–∏ –í–°–ï –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –∏–∑ —Ç–∞–±–ª–∏—Ü—ã Supplement Facts
- –°–æ—Ö—Ä–∞–Ω–∏ —Ç–æ—á–Ω—ã–µ –¥–æ–∑–∏—Ä–æ–≤–∫–∏ —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏ (mg, mcg, IU, etc.)
- –ù–ï –¥–æ–±–∞–≤–ª—è–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –¢–û–õ–¨–ö–û —á–∏—Å—Ç—ã–π JSON"""
            
            # –ó–∞–ø—Ä–æ—Å –∫ OpenAI Vision
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π Supplement Facts –¥–ª—è: {brand} {product_name}"
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=1500,
                temperature=0
            )
            
            self.request_count += 1
            self.total_cost += 0.02
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
            content = response.choices[0].message.content.strip()
            
            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç markdown
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            # –ü–∞—Ä—Å–∏–Ω–≥ JSON
            try:
                data = json.loads(content)
            except json.JSONDecodeError:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç OpenAI: {content}")
                return "", "", "", ""
            
            ingredients = data.get("ingredients", "")
            dosages = data.get("dosages", "")
            age_group = data.get("age_group", "")
            form = data.get("form", "")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            cache_data = {
                "ingredients": ingredients,
                "dosages": dosages,
                "age_group": age_group,
                "form": form,
                "analyzed_at": datetime.now().isoformat(),
                "product_name": product_name,
                "brand": brand
            }
            self._save_cache(image_id, cache_data)
            
            logger.info(f"‚úÖ OpenAI –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –ò–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤: {len(ingredients.split(',')) if ingredients else 0}")
            return ingredients, dosages, age_group, form
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ OpenAI –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return "", "", "", ""
    
    def get_stats(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è OpenAI"""
        return {
            'requests': self.request_count,
            'estimated_cost': self.total_cost
        }


class GoogleDrivePipelineProcessor:
    """Pipeline –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π –≤ Google Drive"""
    
    def __init__(self, rainforest_api_key: str, openai_api_key: str, 
                 google_credentials_path: str, output_dir: str = "output"):
        self.rainforest_api_key = rainforest_api_key
        self.base_url = "https://api.rainforestapi.com/request"
        
        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        logger.info("üîç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OCR...")
        self.ocr_reader = easyocr.Reader(['en'], gpu=False, verbose=False)
        
        logger.info("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI Vision...")
        self.ai_analyzer = OpenAISupplementFactsAnalyzer(openai_api_key)
        
        logger.info("‚òÅÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Google Drive...")
        self.drive_uploader = GoogleDriveUploader(google_credentials_path)
        
        # –°–µ—Å—Å–∏—è –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–æ–≤
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        })
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ Supplement Facts
        self.supplement_keywords = [
            'supplement facts', 'supplement fact', 'nutrition facts',
            'serving size', 'servings per container', 'amount per serving',
            'daily value', '% daily value', '%dv', 'vitamin', 'mineral'
        ]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_api_calls': 0,
            'search_calls': 0,
            'product_calls': 0,
            'rainforest_credits': 0,
            'products_found': 0,
            'supplement_facts_found': 0,
            'successful_extractions': 0,
            'drive_uploads': 0,
            'start_time': datetime.now()
        }
        
        logger.info("‚úÖ Google Drive Pipeline –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"üìÅ –ü–∞–ø–∫–∞ –≤ Google Drive: {self.drive_uploader.get_folder_url()}")

    def load_existing_data(self, output_file: str) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –ø—É—Å—Ç—É—é —Ç–∞–±–ª–∏—Ü—É"""
        try:
            df = pd.read_csv(output_file, dtype=str).fillna("")
            logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è —Ç–∞–±–ª–∏—Ü–∞: {len(df)} –∑–∞–ø–∏—Å–µ–π")
            return df
        except FileNotFoundError:
            logger.info(f"üìÇ –§–∞–π–ª {output_file} –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞")
            return pd.DataFrame()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {output_file}: {e}")
            return pd.DataFrame()
    
    def check_search_term_processed(self, df: pd.DataFrame, search_term: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ª–∏ —É–∂–µ –¥–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"""
        if df.empty or 'Search Term' not in df.columns:
            return False
        
        existing_records = df[df['Search Term'] == search_term]
        if len(existing_records) > 0:
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(existing_records)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è '{search_term}'")
            return True
        return False
    
    def is_product_processed(self, row) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ª–∏ —É–∂–µ —Ç–æ–≤–∞—Ä"""
        bsr_filled = bool(str(row.get('BSR', '')).strip() and str(row.get('BSR', '')).strip() != 'nan')
        category_filled = bool(str(row.get('–ö–∞—Ç–µ–≥–æ—Ä–∏—è', '')).strip())
        
        return bsr_filled or category_filled
    
    def create_products_dataframe(self, products: List[Dict], search_term: str, existing_df: pd.DataFrame = None) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç DataFrame —Å —Ç–æ–≤–∞—Ä–∞–º–∏ –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π"""
        
        logger.info(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ ({len(products)} –∑–∞–ø–∏—Å–µ–π) –¥–ª—è '{search_term}'")
        
        if existing_df is not None and not existing_df.empty:
            df = existing_df.copy()
            start_index = len(df) + 1
        else:
            df = pd.DataFrame()
            start_index = 1
        
        records = []
        for i, product in enumerate(products, start_index):
            price_raw = ""
            if product['price']:
                price_raw = product['price'].get('raw', '')
            
            bsr_info = ""
            if product['bestsellers_rank']:
                bsr_list = []
                for rank in product['bestsellers_rank']:
                    category = rank.get('category', 'Unknown')
                    rank_num = rank.get('rank', 'N/A')
                    bsr_list.append(f"{category}: #{rank_num}")
                bsr_info = "; ".join(bsr_list)
            
            record = {
                '‚Ññ': i,
                'Search Term': search_term,
                'ASIN': product['asin'],
                '–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞ (Title)': product['title'],
                '–ë—Ä–µ–Ω–¥': product['brand'],
                '–¶–µ–Ω–∞ (USD)': price_raw,
                '–í–æ–∑—Ä–∞—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞': '',
                '–§–æ—Ä–º–∞ –≤—ã–ø—É—Å–∫–∞': '',
                '–í—Å–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã (–∏–∑ Supplement Facts)': '',
                '–î–æ–∑–∏—Ä–æ–≤–∫–∏ (–º–≥/–µ–¥.)': '',
                'Claims (sugar free, organic –∏ —Ç.–¥.)': '',
                '–ö–æ–ª-–≤–æ –æ—Ç–∑—ã–≤–æ–≤': product['ratings_total'],
                '–†–µ–π—Ç–∏–Ω–≥': product['rating'],
                'BSR': bsr_info,
                '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '',
                '–°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä': product['link'],
                '–°—Å—ã–ª–∫–∞ –Ω–∞ Supplement Facts (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)': ''
            }
            records.append(record)
        
        new_df = pd.DataFrame(records)
        if not df.empty:
            df = pd.concat([df, new_df], ignore_index=True)
        else:
            df = new_df
        
        logger.info(f"‚úÖ –¢–∞–±–ª–∏—Ü–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {len(new_df)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π, –≤—Å–µ–≥–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        return df
    
    def process_detailed_products(self, df: pd.DataFrame, limit: int = None) -> pd.DataFrame:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä–æ–≤ —Å OpenAI Vision"""
        
        total_products = len(df) if limit is None else min(limit, len(df))
        
        logger.info(f"\nüîç –î–µ—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ (–ª–∏–º–∏—Ç: {total_products})")
        
        processed_count = 0
        skipped_count = 0
        
        for i in range(len(df)):
            if limit and processed_count >= limit:
                logger.info(f"üõë –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏: {limit}")
                break
                
            asin = df.iloc[i]['ASIN']
            
            if self.is_product_processed(df.iloc[i]):
                logger.info(f"‚è≠Ô∏è  –¢–æ–≤–∞—Ä {i+1}: {asin} - —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
                skipped_count += 1
                continue
            
            logger.info(f"\nüì¶ –¢–æ–≤–∞—Ä {i+1}: {asin}")
            
            details = self.get_product_details(asin)
            
            if not details['success']:
                logger.warning(f"      ‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {details.get('error', 'Unknown')}")
                continue
            
            product_data = details['product']
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if not df.iloc[i]['–ë—Ä–µ–Ω–¥'] and product_data.get('brand'):
                df.iloc[i, df.columns.get_loc('–ë—Ä–µ–Ω–¥')] = product_data['brand']
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            categories = product_data.get('categories', [])
            if categories:
                category_names = [cat.get('name', '') for cat in categories]
                df.iloc[i, df.columns.get_loc('–ö–∞—Ç–µ–≥–æ—Ä–∏—è')] = ' > '.join(category_names)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º BSR –∏–∑ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            bestsellers_rank = product_data.get('bestsellers_rank', [])
            if bestsellers_rank:
                first_rank = bestsellers_rank[0]
                bsr_number = first_rank.get('rank', 'N/A')
                bsr_category = first_rank.get('category', 'Unknown')
                
                df.iloc[i, df.columns.get_loc('BSR')] = str(bsr_number)
                logger.info(f"      üìä BSR: #{bsr_number} ({bsr_category})")
            
            # –ò—â–µ–º Supplement Facts
            supplement_image = self.find_supplement_facts_image(product_data)
            
            if supplement_image:
                df.iloc[i, df.columns.get_loc('–°—Å—ã–ª–∫–∞ –Ω–∞ Supplement Facts (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)')] = supplement_image
                logger.info(f"      üéØ Supplement Facts –Ω–∞–π–¥–µ–Ω!")
                
                # OpenAI Vision –∞–Ω–∞–ª–∏–∑
                try:
                    product_title = df.iloc[i]['–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞ (Title)']
                    brand = df.iloc[i]['–ë—Ä–µ–Ω–¥']
                    
                    ingredients, dosages, age_group, form = self.ai_analyzer.analyze_supplement_facts(
                        supplement_image, product_title, brand
                    )
                    
                    self.stats['successful_extractions'] += 1
                    
                    # –ó–∞–ø–æ–ª–Ω—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    df.iloc[i, df.columns.get_loc('–í—Å–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã (–∏–∑ Supplement Facts)')] = ingredients
                    df.iloc[i, df.columns.get_loc('–î–æ–∑–∏—Ä–æ–≤–∫–∏ (–º–≥/–µ–¥.)')] = dosages
                    
                    if age_group:
                        df.iloc[i, df.columns.get_loc('–í–æ–∑—Ä–∞—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞')] = age_group
                        logger.info(f"         üìÖ –í–æ–∑—Ä–∞—Å—Ç: {age_group}")
                    
                    if form:
                        df.iloc[i, df.columns.get_loc('–§–æ—Ä–º–∞ –≤—ã–ø—É—Å–∫–∞')] = form
                        logger.info(f"         üíä –§–æ—Ä–º–∞: {form}")
                    
                    logger.info(f"      ‚úÖ OpenAI Vision –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
                    
                except Exception as e:
                    logger.error(f"      ‚ùå –û—à–∏–±–∫–∞ OpenAI –∞–Ω–∞–ª–∏–∑–∞: {e}")
                
            else:
                logger.warning(f"      ‚ùå Supplement Facts –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            processed_count += 1
            time.sleep(1)
        
        logger.info(f"\nüìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {processed_count} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ, {skipped_count} –ø—Ä–æ–ø—É—â–µ–Ω–æ")
        return df
    
    def get_product_details(self, asin: str) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞"""
        
        logger.info(f"   üì¶ –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: {asin}")
        
        params = {
            'api_key': self.rainforest_api_key,
            'type': 'product',
            'amazon_domain': 'amazon.com',
            'asin': asin
        }
        
        try:
            response = self.session.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            self.stats['total_api_calls'] += 1
            self.stats['product_calls'] += 1
            
            if 'request_info' in data:
                credits_used = data['request_info'].get('credits_used_this_request', 1)
                self.stats['rainforest_credits'] += credits_used
                logger.info(f"      üí≥ –ö—Ä–µ–¥–∏—Ç–æ–≤: {credits_used}")
            
            if not data.get('request_info', {}).get('success', False):
                return {'success': False, 'error': 'API request failed'}
            
            product_data = data.get('product', {})
            
            if not product_data:
                return {'success': False, 'error': 'Product data not found'}
            
            logger.info(f"      ‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã")
            return {'success': True, 'product': product_data}
            
        except Exception as e:
            logger.error(f"      ‚ùå –û—à–∏–±–∫–∞: {e}")
            return {'success': False, 'error': str(e)}
    
    def analyze_image_for_supplement_facts(self, image_url: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ Supplement Facts (OCR –≤–∞–ª–∏–¥–∞—Ü–∏—è)"""
        result = {
            'url': image_url,
            'accessible': False,
            'contains_supplement_facts': False,
            'confidence': 0.0,
            'keywords_found': [],
            'error': None
        }
        
        try:
            response = requests.get(image_url, timeout=10, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            if response.status_code != 200:
                result['error'] = f"HTTP {response.status_code}"
                return result
            
            result['accessible'] = True
            
            # OCR –∞–Ω–∞–ª–∏–∑
            image = Image.open(io.BytesIO(response.content))
            
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            image_array = np.array(image)
            ocr_results = self.ocr_reader.readtext(image_array)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
            full_text = ' '.join([item[1] for item in ocr_results]).lower()
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            found_keywords = []
            confidence_score = 0.0
            
            for keyword in self.supplement_keywords:
                if keyword in full_text:
                    found_keywords.append(keyword)
                    if keyword == 'supplement facts':
                        confidence_score += 0.5
                    elif keyword in ['serving size', 'servings per container']:
                        confidence_score += 0.2
                    elif keyword in ['daily value', '% daily value', '%dv']:
                        confidence_score += 0.15
                    else:
                        confidence_score += 0.05
            
            result['keywords_found'] = found_keywords
            result['confidence'] = min(confidence_score, 1.0)
            result['contains_supplement_facts'] = confidence_score >= 0.3
            
        except Exception as e:
            result['error'] = str(e)
        
        return result
    
    def find_supplement_facts_image(self, product_data: Dict) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å Supplement Facts"""
        
        images = product_data.get('images', [])
        if not images:
            return None
            
        logger.info(f"      üîç –ê–Ω–∞–ª–∏–∑ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        
        best_image = None
        best_confidence = 0.0
        
        for image_data in images:
            image_url = image_data.get('link', '')
            
            if not image_url:
                continue
            
            analysis = self.analyze_image_for_supplement_facts(image_url)
            
            if analysis['accessible'] and analysis['contains_supplement_facts']:
                logger.info(f"         üéâ –ù–ê–ô–î–ï–ù! –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {analysis['confidence']:.2f}")
                
                if analysis['confidence'] > best_confidence:
                    best_confidence = analysis['confidence']
                    best_image = image_url
        
        if best_image:
            self.stats['supplement_facts_found'] += 1
            return best_image
        
        return None

    # –ú–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∫–æ–ø–∏—Ä—É–µ–º –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ pipeline)
    def load_keywords(self, keywords_file: str) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ CSV —Ñ–∞–π–ª–∞"""
        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {keywords_file}")
        
        try:
            if not os.path.exists(keywords_file):
                raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {keywords_file}")
            
            df = pd.read_csv(keywords_file)
            keywords = df.iloc[:, 1].dropna().tolist()
            
            if not keywords:
                raise ValueError("–ö–ª—é—á–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(keywords)} –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
            return keywords
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            return []

    def search_products_multiple_pages(self, search_term: str, max_pages: int = 2) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        
        logger.info(f"üîç –ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤: '{search_term}' (–¥–æ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü)")
        
        all_products = []
        
        for page in range(1, max_pages + 1):
            logger.info(f"   üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}/{max_pages}")
            
            params = {
                'api_key': self.rainforest_api_key,
                'type': 'search',
                'amazon_domain': 'amazon.com',
                'search_term': search_term,
                'page': page
            }
            
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = self.session.get(self.base_url, params=params, timeout=60)
                    response.raise_for_status()
                    
                    data = response.json()
                    
                    self.stats['total_api_calls'] += 1
                    self.stats['search_calls'] += 1
                    
                    if 'request_info' in data:
                        credits = data['request_info'].get('credits_used_this_request', 1)
                        self.stats['rainforest_credits'] += credits
                        logger.info(f"      üí≥ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤: {credits}")
                    
                    if not data.get('request_info', {}).get('success', False):
                        error_msg = data.get('request_info', {}).get('message', 'Unknown error')
                        logger.error(f"      ‚ùå API –æ—à–∏–±–∫–∞: {error_msg}")
                        break
                    
                    search_results = data.get('search_results', [])
                    logger.info(f"      ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(search_results)}")
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    for result in search_results:
                        product = {
                            'asin': result.get('asin', ''),
                            'title': result.get('title', ''),
                            'link': result.get('link', ''),
                            'rating': result.get('rating', 0),
                            'ratings_total': result.get('ratings_total', 0),
                            'price': result.get('price', {}),
                            'image': result.get('image', ''),
                            'is_prime': result.get('is_prime', False),
                            'brand': result.get('brand', ''),
                            'bestsellers_rank': result.get('bestsellers_rank', []),
                            'search_term': search_term,
                            'page_found': page
                        }
                        all_products.append(product)
                    
                    break  # –£—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å
                    
                except Exception as e:
                    logger.warning(f"      ‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                    if attempt == max_retries - 1:
                        logger.error(f"      ‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}")
                    else:
                        time.sleep(2 ** attempt)
            
            if page < max_pages:
                time.sleep(2)
        
        self.stats['products_found'] += len(all_products)
        logger.info(f"   üéØ –ò—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(all_products)}")
        return all_products

    def save_results_to_drive(self, df: pd.DataFrame, filename: str) -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª–æ–∫–∞–ª—å–Ω–æ –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤ Google Drive"""
        try:
            # –°–æ–∑–¥–∞–µ–º timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # –õ–æ–∫–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            local_file = self.output_dir / filename
            df.to_csv(local_file, index=False, encoding='utf-8')
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Google Drive
            drive_filename = f"kids_supplements_{timestamp}.csv"
            drive_url = self.drive_uploader.upload_file(str(local_file), drive_filename)
            
            if drive_url:
                self.stats['drive_uploads'] += 1
                logger.info(f"‚òÅÔ∏è –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤ Google Drive: {drive_url}")
                
                # –¢–∞–∫–∂–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª (–±–µ–∑ timestamp)
                self.drive_uploader.upload_file(str(local_file), "kids_supplements_latest.csv")
                
                return drive_url
            else:
                logger.error("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Google Drive")
                return ""
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            return ""

    def print_final_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–∫–ª—é—á–∞—è Google Drive"""
        duration = datetime.now() - self.stats['start_time']
        openai_stats = self.ai_analyzer.get_stats()
        
        print("\n" + "=" * 80)
        print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê GOOGLE DRIVE PIPELINE")
        print("=" * 80)
        print(f"üïê –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {duration}")
        print(f"üìû –í—Å–µ–≥–æ API –≤—ã–∑–æ–≤–æ–≤: {self.stats['total_api_calls']}")
        print(f"üîç –ü–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {self.stats['search_calls']}")
        print(f"üì¶ –ó–∞–ø—Ä–æ—Å–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤: {self.stats['product_calls']}")
        print(f"üí≥ Rainforest –∫—Ä–µ–¥–∏—Ç–æ–≤: {self.stats['rainforest_credits']}")
        print(f"ü§ñ OpenAI –∑–∞–ø—Ä–æ—Å–æ–≤: {openai_stats['requests']}")
        print(f"üí∞ –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å OpenAI: ${openai_stats['estimated_cost']:.2f}")
        print(f"‚òÅÔ∏è –ó–∞–≥—Ä—É–∑–æ–∫ –≤ Google Drive: {self.stats['drive_uploads']}")
        print(f"üõçÔ∏è –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {self.stats['products_found']}")
        print(f"üéØ –ù–∞–π–¥–µ–Ω–æ Supplement Facts: {self.stats['supplement_facts_found']}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏–π: {self.stats['successful_extractions']}")
        print(f"üìÅ –ü–∞–ø–∫–∞ Google Drive: {self.drive_uploader.get_folder_url()}")
        print("=" * 80)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Google Drive pipeline"""
    parser = argparse.ArgumentParser(description='Pipeline —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π –≤ Google Drive')
    parser.add_argument('--rainforest-key', required=True, help='Rainforest API –∫–ª—é—á')
    parser.add_argument('--openai-key', required=True, help='OpenAI API –∫–ª—é—á')
    parser.add_argument('--google-credentials', required=True, help='–ü—É—Ç—å –∫ Google credentials.json')
    parser.add_argument('--keywords-file', default='Kids Supplements Keywords.csv', help='–§–∞–π–ª —Å –∫–ª—é—á–µ–≤—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏')
    parser.add_argument('--output-dir', default='output', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤')
    parser.add_argument('--max-pages', type=int, default=2, help='–ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–∏—Å–∫–∞')
    parser.add_argument('--detail-limit', type=int, default=10, help='–õ–∏–º–∏—Ç —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--keyword-limit', type=int, default=None, help='–õ–∏–º–∏—Ç –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤')
    
    args = parser.parse_args()
    
    logger.info("üöÄ –ó–ê–ü–£–°–ö GOOGLE DRIVE PIPELINE")
    logger.info("=" * 80)
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        processor = GoogleDrivePipelineProcessor(
            args.rainforest_key, 
            args.openai_key,
            args.google_credentials,
            args.output_dir
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        keywords = processor.load_keywords(args.keywords_file)
        if not keywords:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã")
            return 1
        
        if args.keyword_limit:
            keywords = keywords[:args.keyword_limit]
            logger.info(f"üéØ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(keywords)} –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        output_file = 'kids_supplements.csv'
        existing_df = processor.load_existing_data(output_file)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        df = existing_df
        
        for keyword_idx, search_term in enumerate(keywords):
            logger.info(f"\nüéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª—é—á–µ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ {keyword_idx+1}/{len(keywords)}: '{search_term}'")
            
            if not existing_df.empty and processor.check_search_term_processed(existing_df, search_term):
                logger.info(f"‚è≠Ô∏è  –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å '{search_term}' —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫")
            else:
                products = processor.search_products_multiple_pages(search_term, args.max_pages)
                
                if not products:
                    logger.warning(f"‚ùå –¢–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è '{search_term}'")
                    continue
                
                df = processor.create_products_dataframe(products, search_term, df)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Google Drive
                drive_url = processor.save_results_to_drive(df, output_file)
                logger.info(f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {len(df)} –∑–∞–ø–∏—Å–µ–π ‚Üí {drive_url}")
        
        if df.empty:
            logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return 1
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (Supplement Facts —Å OpenAI)
        df = processor.process_detailed_products(df, args.detail_limit)
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Google Drive
        drive_url = processor.save_results_to_drive(df, output_file)
        
        logger.info(f"\nüéâ GOOGLE DRIVE PIPELINE –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
        logger.info(f"‚òÅÔ∏è –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Google Drive: {drive_url}")
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("\n‚èπÔ∏è Pipeline –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return 1
    except Exception as e:
        logger.error(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1
    finally:
        if 'processor' in locals():
            processor.print_final_stats()


if __name__ == "__main__":
    sys.exit(main())
