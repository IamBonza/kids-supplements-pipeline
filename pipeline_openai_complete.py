#!/usr/bin/env python3
"""
–ü–æ–ª–Ω—ã–π pipeline –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–µ—Ç—Å–∫–∏—Ö –≤–∏—Ç–∞–º–∏–Ω–æ–≤ —Å OpenAI Vision API:
1. –ß–∏—Ç–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
2. –î–µ–ª–∞–µ—Ç –ø–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)  
3. –ó–∞–ø–æ–ª–Ω—è–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
4. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å Supplement Facts —á–µ—Ä–µ–∑ OpenAI Vision
"""

import pandas as pd
import requests
import easyocr
from PIL import Image
import io
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import argparse
import sys
import json
import time
from datetime import datetime
import os
import re
import openai
import base64

class OpenAISupplementFactsAI:
    """–ö–ª–∞—Å—Å –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞ Supplement Facts –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ OpenAI Vision API"""
    
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        self.known_extractions = {
            # –ö—ç—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        }
    
    def extract_image_id(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç ID –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ Amazon URL"""
        try:
            match = re.search(r'/([A-Z0-9]{10,11})[._-]', url)
            if match:
                return match.group(1)
            return ""
        except:
            return ""
    
    def download_image(self, url: str) -> bytes:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ URL"""
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        return response.content
    
    def encode_image_base64(self, image_bytes: bytes) -> str:
        """–ö–æ–¥–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64"""
        return base64.b64encode(image_bytes).decode('utf-8')
    
    def analyze_supplement_facts(self, url: str, product_name: str = "", brand: str = "") -> tuple:
        """
        AI –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ OpenAI Vision.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (ingredients, dosages, age_group, form)
        """
        print(f"         ü§ñ OpenAI Vision –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç Supplement Facts...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        image_id = self.extract_image_id(url)
        if image_id in self.known_extractions:
            print(f"         ‚úÖ –ù–∞–π–¥–µ–Ω –≤ –∫—ç—à–µ: {image_id}")
            result = self.known_extractions[image_id]
            return (
                result["ingredients"], 
                result["dosages"],
                result.get("age_group", ""),
                result.get("form", "")
            )
        
        try:
            # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_bytes = self.download_image(url)
            image_base64 = self.encode_image_base64(image_bytes)
            
            # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            system_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —ç—Ç–∏–∫–µ—Ç–æ–∫ –ø–∏—â–µ–≤—ã—Ö –¥–æ–±–∞–≤–æ–∫. –ò–∑–≤–ª–µ–∫–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è Supplement Facts –∏ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ:

{
  "ingredients": "—Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é",
  "dosages": "–ø–∞—Ä—ã –ò–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç: –¥–æ–∑–∏—Ä–æ–≤–∫–∞ —á–µ—Ä–µ–∑ —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π",
  "age_group": "–≤–æ–∑—Ä–∞—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ 2+, 4+ –∏ —Ç.–¥.",
  "form": "—Ñ–æ—Ä–º–∞ –≤—ã–ø—É—Å–∫–∞: Gummies, Chewable, Tablets, Capsules, Liquid, Drops, Powder, Softgels"
}

–ï—Å–ª–∏ –∫–∞–∫–æ–µ-—Ç–æ –ø–æ–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –æ—Å—Ç–∞–≤—å –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É. –ù–ï –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –¢–û–õ–¨–ö–û JSON."""
            
            # –ó–∞–ø—Ä–æ—Å –∫ OpenAI
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system", 
                        "content": system_prompt
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π Supplement Facts –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞: {brand} {product_name}"
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=1000,
                temperature=0
            )
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            content = response.choices[0].message.content.strip()
            
            # –£–±–∏—Ä–∞–µ–º markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            # –ü–∞—Ä—Å–∏–º JSON
            data = json.loads(content)
            
            ingredients = data.get("ingredients", "")
            dosages = data.get("dosages", "")
            age_group = data.get("age_group", "")
            form = data.get("form", "")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            if image_id:
                self.known_extractions[image_id] = {
                    "ingredients": ingredients,
                    "dosages": dosages,
                    "age_group": age_group,
                    "form": form
                }
            
            print(f"         ‚úÖ OpenAI –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
            return ingredients, dosages, age_group, form
            
        except json.JSONDecodeError as e:
            print(f"         ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return "", "", "", ""
            
        except Exception as e:
            print(f"         ‚ùå –û—à–∏–±–∫–∞ OpenAI –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return "", "", "", ""

class FullPipelineProcessor:
    """–ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏—Ç–∞–º–∏–Ω–æ–≤ —Å OpenAI Vision"""
    
    def __init__(self, rainforest_api_key: str, openai_api_key: str):
        self.rainforest_api_key = rainforest_api_key
        self.openai_api_key = openai_api_key
        self.base_url = "https://api.rainforestapi.com/request"
        
        print("üîç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OCR...")
        self.ocr_reader = easyocr.Reader(['en'], gpu=False)
        print("‚úÖ OCR –≥–æ—Ç–æ–≤")
        
        print("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI Vision...")
        self.ai_analyzer = OpenAISupplementFactsAI(openai_api_key)
        print("‚úÖ OpenAI Vision –≥–æ—Ç–æ–≤")
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        })
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ Supplement Facts
        self.supplement_keywords = [
            'supplement facts', 'supplement fact', 'nutrition facts',
            'serving size', 'servings per container', 'amount per serving',
            'daily value', '% daily value', '%dv'
        ]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_api_calls': 0,
            'search_calls': 0,
            'product_calls': 0,
            'credits_used': 0,
            'products_found': 0,
            'supplement_facts_found': 0,
            'openai_calls': 0,
            'start_time': datetime.now()
        }
    
    def load_keywords(self, keywords_file: str) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ CSV —Ñ–∞–π–ª–∞"""
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑: {keywords_file}")
        
        try:
            df = pd.read_csv(keywords_file)
            keywords = df.iloc[:, 1].dropna().tolist()
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(keywords)} –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
            return keywords
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            return []
    
    def search_products_multiple_pages(self, search_term: str, max_pages: int = 2) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π"""
        
        print(f"\nüîç –ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤: '{search_term}' (–¥–æ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü)")
        
        all_products = []
        
        for page in range(1, max_pages + 1):
            print(f"   ÔøΩÔøΩ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}/{max_pages}")
            
            params = {
                'api_key': self.rainforest_api_key,
                'type': 'search',
                'amazon_domain': 'amazon.com',
                'search_term': search_term,
                'page': page
            }
            
            try:
                response = self.session.get(self.base_url, params=params, timeout=30)
                response.raise_for_status()
                
                data = response.json()
                
                self.stats['total_api_calls'] += 1
                self.stats['search_calls'] += 1
                
                if 'request_info' in data:
                    credits_used = data['request_info'].get('credits_used_this_request', 1)
                    self.stats['credits_used'] += credits_used
                    print(f"      üí≥ –ö—Ä–µ–¥–∏—Ç–æ–≤: {credits_used}")
                
                if not data.get('request_info', {}).get('success', False):
                    print(f"      ‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {data}")
                    continue
                
                search_results = data.get('search_results', [])
                print(f"      ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(search_results)} —Ç–æ–≤–∞—Ä–æ–≤")
                
                for result in search_results:
                    product = {
                        'asin': result.get('asin', ''),
                        'title': result.get('title', ''),
                        'link': result.get('link', ''),
                        'rating': result.get('rating', 0),
                        'ratings_total': result.get('ratings_total', 0),
                        'price': result.get('price', {}),
                        'image': result.get('image', ''),
                        'is_prime': result.get('is_prime', False),
                        'brand': result.get('brand', ''),
                        'bestsellers_rank': result.get('bestsellers_rank', []),
                        'search_term': search_term,
                        'page_found': page
                    }
                    all_products.append(product)
                
                if page < max_pages:
                    time.sleep(1)
                    
            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")
                continue
        
        self.stats['products_found'] += len(all_products)
        print(f"   üéØ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(all_products)}")
        return all_products
    
    def get_product_details(self, asin: str) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞"""
        
        print(f"   üì¶ –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: {asin}")
        
        params = {
            'api_key': self.rainforest_api_key,
            'type': 'product',
            'amazon_domain': 'amazon.com',
            'asin': asin
        }
        
        try:
            response = self.session.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            self.stats['total_api_calls'] += 1
            self.stats['product_calls'] += 1
            
            if 'request_info' in data:
                credits_used = data['request_info'].get('credits_used_this_request', 1)
                self.stats['credits_used'] += credits_used
                print(f"      üí≥ –ö—Ä–µ–¥–∏—Ç–æ–≤: {credits_used}")
            
            if not data.get('request_info', {}).get('success', False):
                return {'success': False, 'error': 'API request failed'}
            
            product_data = data.get('product', {})
            
            if not product_data:
                return {'success': False, 'error': 'Product data not found'}
            
            print(f"      ‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã")
            return {'success': True, 'product': product_data}
            
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞: {e}")
            return {'success': False, 'error': str(e)}
    
    def analyze_image_for_supplement_facts(self, image_url: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ Supplement Facts (OCR –≤–∞–ª–∏–¥–∞—Ü–∏—è)"""
        result = {
            'url': image_url,
            'accessible': False,
            'contains_supplement_facts': False,
            'confidence': 0.0,
            'keywords_found': [],
            'error': None
        }
        
        try:
            response = self.session.get(image_url, timeout=10)
            response.raise_for_status()
            
            result['accessible'] = True
            
            if len(response.content) < 10000:
                result['error'] = '–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π'
                return result
            
            # OCR –∞–Ω–∞–ª–∏–∑
            image = Image.open(io.BytesIO(response.content))
            image_np = np.array(image)
            
            ocr_results = self.ocr_reader.readtext(image_np)
            all_text = ' '.join([result[1] for result in ocr_results]).lower()
            
            # –ü–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            found_keywords = []
            confidence = 0.0
            
            if 'supplement facts' in all_text:
                confidence += 0.9
                found_keywords.append('supplement facts')
            elif 'supplement fact' in all_text:
                confidence += 0.8
                found_keywords.append('supplement fact')
            
            additional = [
                ('serving size', 0.2), ('servings per container', 0.2),
                ('daily value', 0.15), ('% daily value', 0.15),
                ('amount per serving', 0.1)
            ]
            
            for keyword, weight in additional:
                if keyword in all_text:
                    confidence += weight
                    found_keywords.append(keyword)
            
            result['confidence'] = min(confidence, 1.0)
            result['keywords_found'] = found_keywords
            result['contains_supplement_facts'] = confidence > 0.6
            
            return result
            
        except Exception as e:
            result['error'] = str(e)
            return result
    
    def find_supplement_facts_image(self, product_data: Dict) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å Supplement Facts"""
        
        images = product_data.get('images', [])
        if not images:
            return None
            
        print(f"      üîç –ê–Ω–∞–ª–∏–∑ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        
        best_image = None
        best_confidence = 0.0
        
        for image_data in images:
            image_url = image_data.get('link', '')
            
            if not image_url:
                continue
            
            analysis = self.analyze_image_for_supplement_facts(image_url)
            
            if analysis['accessible'] and analysis['contains_supplement_facts']:
                print(f"         üéâ –ù–ê–ô–î–ï–ù! –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {analysis['confidence']:.2f}")
                
                if analysis['confidence'] > best_confidence:
                    best_confidence = analysis['confidence']
                    best_image = image_url
        
        if best_image:
            self.stats['supplement_facts_found'] += 1
            return best_image
        
        return None
    
    def load_existing_data(self, output_file: str) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –ø—É—Å—Ç—É—é —Ç–∞–±–ª–∏—Ü—É"""
        try:
            df = pd.read_csv(output_file, dtype=str).fillna("")
            print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è —Ç–∞–±–ª–∏—Ü–∞: {len(df)} –∑–∞–ø–∏—Å–µ–π")
            return df
        except FileNotFoundError:
            print(f"üìÇ –§–∞–π–ª {output_file} –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞")
            return pd.DataFrame()
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {output_file}: {e}")
            return pd.DataFrame()
    
    def check_search_term_processed(self, df: pd.DataFrame, search_term: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ª–∏ —É–∂–µ –¥–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"""
        if df.empty or 'Search Term' not in df.columns:
            return False
        
        existing_records = df[df['Search Term'] == search_term]
        if len(existing_records) > 0:
            print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(existing_records)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è '{search_term}'")
            return True
        return False
    
    def create_products_dataframe(self, products: List[Dict], search_term: str, existing_df: pd.DataFrame = None) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç DataFrame —Å —Ç–æ–≤–∞—Ä–∞–º–∏ –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π"""
        
        print(f"\nÔøΩÔøΩ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ ({len(products)} –∑–∞–ø–∏—Å–µ–π) –¥–ª—è '{search_term}'")
        
        if existing_df is not None and not existing_df.empty:
            df = existing_df.copy()
            start_index = len(df) + 1
        else:
            df = pd.DataFrame()
            start_index = 1
        
        records = []
        for i, product in enumerate(products, start_index):
            price_raw = ""
            if product['price']:
                price_raw = product['price'].get('raw', '')
            
            bsr_info = ""
            if product['bestsellers_rank']:
                bsr_list = []
                for rank in product['bestsellers_rank']:
                    category = rank.get('category', 'Unknown')
                    rank_num = rank.get('rank', 'N/A')
                    bsr_list.append(f"{category}: #{rank_num}")
                bsr_info = "; ".join(bsr_list)
            
            record = {
                '‚Ññ': i,
                'Search Term': search_term,
                'ASIN': product['asin'],
                '–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞ (Title)': product['title'],
                '–ë—Ä–µ–Ω–¥': product['brand'],
                '–¶–µ–Ω–∞ (USD)': price_raw,
                '–í–æ–∑—Ä–∞—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞': '',
                '–§–æ—Ä–º–∞ –≤—ã–ø—É—Å–∫–∞': '',
                '–í—Å–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã (–∏–∑ Supplement Facts)': '',
                '–î–æ–∑–∏—Ä–æ–≤–∫–∏ (–º–≥/–µ–¥.)': '',
                'Claims (sugar free, organic –∏ —Ç.–¥.)': '',
                '–ö–æ–ª-–≤–æ –æ—Ç–∑—ã–≤–æ–≤': product['ratings_total'],
                '–†–µ–π—Ç–∏–Ω–≥': product['rating'],
                'BSR': bsr_info,
                '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '',
                '–°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä': product['link'],
                '–°—Å—ã–ª–∫–∞ –Ω–∞ Supplement Facts (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)': ''
            }
            records.append(record)
        
        new_df = pd.DataFrame(records)
        if not df.empty:
            df = pd.concat([df, new_df], ignore_index=True)
        else:
            df = new_df
        
        print(f"‚úÖ –¢–∞–±–ª–∏—Ü–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {len(new_df)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π, –≤—Å–µ–≥–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        return df
    
    def is_product_processed(self, row) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ª–∏ —É–∂–µ —Ç–æ–≤–∞—Ä"""
        bsr_filled = bool(str(row.get('BSR', '')).strip() and str(row.get('BSR', '')).strip() != 'nan')
        category_filled = bool(str(row.get('–ö–∞—Ç–µ–≥–æ—Ä–∏—è', '')).strip())
        
        return bsr_filled or category_filled
    
    def process_detailed_products(self, df: pd.DataFrame, limit: int = None) -> pd.DataFrame:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä–æ–≤ —Å OpenAI Vision"""
        
        total_products = len(df) if limit is None else min(limit, len(df))
        
        print(f"\nüîç –î–µ—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ (–ª–∏–º–∏—Ç: {total_products})")
        
        processed_count = 0
        skipped_count = 0
        
        for i in range(len(df)):
            if limit and processed_count >= limit:
                print(f"üõë –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏: {limit}")
                break
                
            asin = df.iloc[i]['ASIN']
            
            if self.is_product_processed(df.iloc[i]):
                print(f"‚è≠Ô∏è  –¢–æ–≤–∞—Ä {i+1}: {asin} - —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
                skipped_count += 1
                continue
            
            print(f"\nüì¶ –¢–æ–≤–∞—Ä {i+1}: {asin}")
            
            details = self.get_product_details(asin)
            
            if not details['success']:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {details.get('error', 'Unknown')}")
                continue
            
            product_data = details['product']
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if not df.iloc[i]['–ë—Ä–µ–Ω–¥'] and product_data.get('brand'):
                df.iloc[i, df.columns.get_loc('–ë—Ä–µ–Ω–¥')] = product_data['brand']
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            categories = product_data.get('categories', [])
            if categories:
                category_names = [cat.get('name', '') for cat in categories]
                df.iloc[i, df.columns.get_loc('–ö–∞—Ç–µ–≥–æ—Ä–∏—è')] = ' > '.join(category_names)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º BSR –∏–∑ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            bestsellers_rank = product_data.get('bestsellers_rank', [])
            if bestsellers_rank:
                first_rank = bestsellers_rank[0]
                bsr_number = first_rank.get('rank', 'N/A')
                bsr_category = first_rank.get('category', 'Unknown')
                
                df.iloc[i, df.columns.get_loc('BSR')] = str(bsr_number)
                print(f"      üìä BSR: #{bsr_number} ({bsr_category})")
            
            # –ò—â–µ–º Supplement Facts
            supplement_image = self.find_supplement_facts_image(product_data)
            
            if supplement_image:
                df.iloc[i, df.columns.get_loc('–°—Å—ã–ª–∫–∞ –Ω–∞ Supplement Facts (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)')] = supplement_image
                print(f"      üéØ Supplement Facts –Ω–∞–π–¥–µ–Ω!")
                
                # OpenAI Vision –∞–Ω–∞–ª–∏–∑
                try:
                    product_title = df.iloc[i]['–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞ (Title)']
                    brand = df.iloc[i]['–ë—Ä–µ–Ω–¥']
                    
                    ingredients, dosages, age_group, form = self.ai_analyzer.analyze_supplement_facts(
                        supplement_image, product_title, brand
                    )
                    
                    self.stats['openai_calls'] += 1
                    
                    # –ó–∞–ø–æ–ª–Ω—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    df.iloc[i, df.columns.get_loc('–í—Å–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã (–∏–∑ Supplement Facts)')] = ingredients
                    df.iloc[i, df.columns.get_loc('–î–æ–∑–∏—Ä–æ–≤–∫–∏ (–º–≥/–µ–¥.)')] = dosages
                    
                    if age_group:
                        df.iloc[i, df.columns.get_loc('–í–æ–∑—Ä–∞—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞')] = age_group
                        print(f"         üìÖ –í–æ–∑—Ä–∞—Å—Ç: {age_group}")
                    
                    if form:
                        df.iloc[i, df.columns.get_loc('–§–æ—Ä–º–∞ –≤—ã–ø—É—Å–∫–∞')] = form
                        print(f"         üíä –§–æ—Ä–º–∞: {form}")
                    
                    print(f"      ‚úÖ OpenAI Vision –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
                    
                except Exception as e:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞ OpenAI –∞–Ω–∞–ª–∏–∑–∞: {e}")
                
            else:
                print(f"      ‚ùå Supplement Facts –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            processed_count += 1
            time.sleep(1)
        
        print(f"\nüìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {processed_count} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ, {skipped_count} –ø—Ä–æ–ø—É—â–µ–Ω–æ")
        return df
    
    def save_results(self, df: pd.DataFrame, output_file: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV"""
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤: {output_file}")
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
    
    def print_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        duration = datetime.now() - self.stats['start_time']
        
        print("\n" + "=" * 60)
        print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"üïê –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {duration}")
        print(f"üìû –í—Å–µ–≥–æ API –≤—ã–∑–æ–≤–æ–≤: {self.stats['total_api_calls']}")
        print(f"ÔøΩÔøΩ –ü–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {self.stats['search_calls']}")
        print(f"üì¶ –ó–∞–ø—Ä–æ—Å–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤: {self.stats['product_calls']}")
        print(f"üí≥ –ü–æ—Ç—Ä–∞—á–µ–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ Rainforest: {self.stats['credits_used']}")
        print(f"ü§ñ OpenAI Vision –≤—ã–∑–æ–≤–æ–≤: {self.stats['openai_calls']}")
        print(f"üõçÔ∏è –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {self.stats['products_found']}")
        print(f"üéØ –ù–∞–π–¥–µ–Ω–æ Supplement Facts: {self.stats['supplement_facts_found']}")
        print("=" * 60)

def main():
    parser = argparse.ArgumentParser(description='–ü–æ–ª–Ω—ã–π pipeline —Å OpenAI Vision API')
    parser.add_argument('--rainforest-key', required=True, help='Rainforest API –∫–ª—é—á')
    parser.add_argument('--openai-key', required=True, help='OpenAI API –∫–ª—é—á')
    parser.add_argument('--keywords-file', default='Kids Supplements Keywords.csv', help='–§–∞–π–ª —Å –∫–ª—é—á–µ–≤—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏')
    parser.add_argument('--output-file', default='kids_supplements_openai_filled.csv', help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')
    parser.add_argument('--max-pages', type=int, default=2, help='–ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–∏—Å–∫–∞')
    parser.add_argument('--detail-limit', type=int, default=3, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    
    args = parser.parse_args()
    
    print("üöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û PIPELINE –° OPENAI VISION")
    print("=" * 60)
    
    processor = FullPipelineProcessor(args.rainforest_key, args.openai_key)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        keywords = processor.load_keywords(args.keywords_file)
        if not keywords:
            print("‚ùå –ö–ª—é—á–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        existing_df = processor.load_existing_data('kids_supplements.csv')
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        df = existing_df
        
        for keyword_idx, search_term in enumerate(keywords):
            print(f"\nüéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª—é—á–µ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ {keyword_idx+1}/{len(keywords)}: '{search_term}'")
            
            if not existing_df.empty and processor.check_search_term_processed(existing_df, search_term):
                print(f"‚è≠Ô∏è  –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å '{search_term}' —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫")
            else:
                products = processor.search_products_multiple_pages(search_term, args.max_pages)
                
                if not products:
                    print(f"‚ùå –¢–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è '{search_term}'")
                    continue
                
                df = processor.create_products_dataframe(products, search_term, df)
                processor.save_results(df, 'kids_supplements.csv')
                print(f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {len(df)} –∑–∞–ø–∏—Å–µ–π")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –∫–ª—é—á
            if keyword_idx >= 0:
                break
        
        if df.empty:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        df = processor.process_detailed_products(df, args.detail_limit)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        processor.save_results(df, args.output_file)
        processor.save_results(df, 'kids_supplements.csv')
        
        print(f"\nüéâ PIPELINE –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {args.output_file}")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    finally:
        processor.print_stats()

if __name__ == "__main__":
    main()
